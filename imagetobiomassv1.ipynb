{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.memory_reserved() / 1024**3, \"GB reserved\")\nprint(torch.cuda.memory_allocated() / 1024**3, \"GB allocated\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os, gc, copy\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport timm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score, mean_squared_error#to compare r2\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n# Config\nclass CFG:\n    seed = 42\n    img_size = 224           \n    batch_size = 16           \n    epochs = 15       #ran on sample of 15 but results were better if the no of epochs was increased       \n    n_folds = 5\n    num_workers = 2\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    swin_model_name = \"swin_tiny_patch4_window7_224\"\n    effnet_model_name = \"efficientnet_b1\"\n\n    lr = 2e-4\n    weight_decay = 1e-6\n\n    target_cols = [\"Dry_Green_g\",\"Dry_Dead_g\",\"Dry_Clover_g\",\"GDM_g\",\"Dry_Total_g\"]\n\n    data_dir = Path(\"/kaggle/input/csiro-biomass\")\n\ndef seed_everything(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_everything(CFG.seed)\nprint(\"Device:\", CFG.device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#pivoting the every 5 image to one entry with the five target labels and meta data should be added and fused together (not done in this version )\ntrain = pd.read_csv(CFG.data_dir / \"train.csv\")\ntest = pd.read_csv(CFG.data_dir / \"test.csv\")\n\nprint(\"train (long) rows:\", len(train))\nprint(\"test (long) rows:\", len(test))\n\n\ntrain_wide = train.pivot_table(\n    index=\"image_path\",\n    columns=\"target_name\",\n    values=\"target\"\n).reset_index()\n\ntrain_wide.columns.name = None\ntrain_wide = train_wide.reset_index(drop=True)\n\nprint(train_wide.sample(5))\nprint(\"Shape:\", train_wide.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#kfold valisation splitting \ntrain_wide[\"fold\"] = -1\nkf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\nfor f, (_, val_idx) in enumerate(kf.split(train_wide)):\n    train_wide.loc[val_idx, \"fold\"] = f\nprint(\"Fold counts:\\n\", train_wide[\"fold\"].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#augmentations and flipping cropping and resizing and adding variations \ndef get_train_tfms(size=CFG.img_size):\n    return A.Compose([\n        A.RandomResizedCrop(size=(size, size), scale=(0.5,1.0), ratio=(0.75,1.33), p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.2),\n        A.ColorJitter(0.2,0.2,0.2,0.02, p=0.5),\n        A.GaussNoise(p=0.1),\n        A.Normalize(),\n        ToTensorV2()\n    ])\n\ndef get_valid_tfms(size=CFG.img_size):\n    return A.Compose([\n        A.Resize(height=size, width=size),\n        A.Normalize(),\n        ToTensorV2()\n    ])\n\ntrain_tfms = get_train_tfms()\nvalid_tfms = get_valid_tfms()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass BiomassDataset(Dataset):\n    def __init__(self, df, img_root, transforms, is_test=False):\n        self.df = df.reset_index(drop=True)\n        self.img_root = img_root\n        self.transforms = transforms\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = str(self.img_root / row[\"image_path\"])#retrieving the image \n\n        import cv2\n        img = cv2.imread(img_path)\n        if img is None:#adding  a black image if image not found in the path \n            img = np.zeros((CFG.img_size, CFG.img_size, 3), dtype=np.uint8)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# convert to rgb\n\n        img = self.transforms(image=img)[\"image\"]#apply transformations \n\n        if self.is_test:\n            return img, row[\"sample_id\"]\n\n        target = torch.tensor([row[c] for c in CFG.target_cols], dtype=torch.float32)\n        return img, target\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#use the pretrained model but changed only the head so we get the pure  feature vector to a linear nn model to predict our output 5 target classes\nclass SwinModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(CFG.swin_model_name, pretrained=True, num_classes=0)\n        self.head = nn.Linear(self.backbone.num_features, len(CFG.target_cols))\n\n    def forward(self, x):\n        f = self.backbone(x)\n        return self.head(f)\n\nclass EffNetB1Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(CFG.effnet_model_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n        self.head = nn.Linear(self.backbone.num_features, len(CFG.target_cols))\n\n    def forward(self, x):\n        f = self.backbone(x)\n        return self.head(f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#forward pass shifitng to gpu , loss calc andback propagation done , updating weights using adamw\ndef train_one_epoch(model, loader, optimizer, criterion):\n    model.train()\n    running_loss = 0.0\n    for imgs, targets in tqdm(loader, leave=False):\n        imgs = imgs.to(CFG.device)\n        targets = targets.to(CFG.device)\n\n        optimizer.zero_grad()\n        preds = model(imgs)\n        loss = criterion(preds, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * imgs.size(0)\n    return running_loss / len(loader.dataset)\n#without updating weights , no backprop\ndef valid_one_epoch(model, loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    preds_all = []\n    targets_all = []\n    with torch.no_grad():\n        for imgs, targets in tqdm(loader, leave=False):\n            imgs = imgs.to(CFG.device)\n            targets = targets.to(CFG.device)\n            preds = model(imgs)\n            loss = criterion(preds, targets)\n            running_loss += loss.item() * imgs.size(0)\n            preds_all.append(preds.cpu().numpy())\n            targets_all.append(targets.cpu().numpy())\n    preds_all = np.concatenate(preds_all, axis=0)\n    targets_all = np.concatenate(targets_all, axis=0)\n    return running_loss / len(loader.dataset), preds_all, targets_all\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#run for some epochs \nfold = 0\ntrain_df = train_wide[train_wide.fold != fold].reset_index(drop=True)\nvalid_df = train_wide[train_wide.fold == fold].reset_index(drop=True)\n\ntrain_ds = BiomassDataset(train_df, CFG.data_dir, get_train_tfms(CFG.img_size))\nvalid_ds = BiomassDataset(valid_df, CFG.data_dir, get_valid_tfms(CFG.img_size))\n\ntrain_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\nvalid_loader = DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n\nmodel_swin = SwinModel().to(CFG.device)\noptimizer_swin = optim.AdamW(model_swin.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\nscheduler_swin = optim.lr_scheduler.CosineAnnealingLR(optimizer_swin, T_max=CFG.epochs * len(train_loader))\ncriterion = nn.MSELoss()\n\nbest_val = 1e9\nbest_state_swin = None\n\nfor epoch in range(CFG.epochs):\n    print(f\"[Swin] Epoch {epoch+1}/{CFG.epochs}\")\n    tr_loss = train_one_epoch(model_swin, train_loader, optimizer_swin, criterion)\n    val_loss, preds_swin, targets_swin = valid_one_epoch(model_swin, valid_loader, criterion)\n    scheduler_swin.step()\n    print(f\"  train_loss: {tr_loss:.4f}  valid_loss: {val_loss:.4f}\")\n\n    if val_loss < best_val:\n        best_val = val_loss\n        # save state to CPU memory (avoid saving to disk)\n        best_state_swin = {k: v.cpu().clone() for k, v in model_swin.state_dict().items()}\n        print(\"  saved best swin state (in memory)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if best_state_swin is not None:\n    model_swin.load_state_dict({k: v.to(CFG.device) for k, v in best_state_swin.items()})\n    model_swin.eval()\n    _, preds_swin, targets_swin = valid_one_epoch(model_swin, valid_loader, criterion)\n    print(\"Recomputed Swin preds on valid set.\")\nelse:\n    print(\"Warning: no best_state_swin found — using last epoch weights\")\n    _, preds_swin, targets_swin = valid_one_epoch(model_swin, valid_loader, criterion)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#same for effnet\ntrain_ds_b1 = BiomassDataset(train_df, CFG.data_dir, get_train_tfms(CFG.img_size))\nvalid_ds_b1 = BiomassDataset(valid_df, CFG.data_dir, get_valid_tfms(CFG.img_size))\n\ntrain_loader_b1 = DataLoader(train_ds_b1, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\nvalid_loader_b1 = DataLoader(valid_ds_b1, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n\nmodel_b1 = EffNetB1Model().to(CFG.device)\noptimizer_b1 = optim.AdamW(model_b1.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\nscheduler_b1 = optim.lr_scheduler.CosineAnnealingLR(optimizer_b1, T_max=CFG.epochs * len(train_loader_b1))\ncriterion = nn.MSELoss()\n\nbest_val_b1 = 1e9\nbest_state_b1 = None\n\nfor epoch in range(CFG.epochs):\n    print(f\"[EffNet-B1] Epoch {epoch+1}/{CFG.epochs}\")\n    tr_loss = train_one_epoch(model_b1, train_loader_b1, optimizer_b1, criterion)\n    val_loss, preds_b1, targets_b1 = valid_one_epoch(model_b1, valid_loader_b1, criterion)\n    scheduler_b1.step()\n    print(f\"  train_loss: {tr_loss:.4f}  valid_loss: {val_loss:.4f}\")\n\n    if val_loss < best_val_b1:\n        best_val_b1 = val_loss\n        best_state_b1 = {k: v.cpu().clone() for k, v in model_b1.state_dict().items()}\n        print(\"  saved best effnet-b1 state (in memory)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if best_state_b1 is not None:\n    model_b1.load_state_dict({k: v.to(CFG.device) for k, v in best_state_b1.items()})\n    model_b1.eval()\n    _, preds_b1, targets_b1 = valid_one_epoch(model_b1, valid_loader_b1, criterion)\n    print(\"Recomputed EffNet-B1 preds on valid set.\")\nelse:\n    print(\"Warning: no best_state_b1 found — using last epoch weights\")\n    _, preds_b1, targets_b1 = valid_one_epoch(model_b1, valid_loader_b1, criterion)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def competition_weighted_r2(y_true, y_pred):\n    weights = np.array([0.1, 0.1, 0.1, 0.2, 0.5])\n    ss_res = 0.0\n    ss_tot = 0.0\n    for i in range(len(weights)):\n        y = y_true[:, i]\n        p = y_pred[:, i]\n        w = weights[i]\n        ss_res += w * np.sum((y - p)**2)\n        ss_tot += w * np.sum((y - np.mean(y))**2)\n    return 1 - (ss_res / ss_tot)\n\ndef print_metrics(name, y_true, y_pred):\n    y_true = np.nan_to_num(y_true)\n    y_pred = np.nan_to_num(y_pred)\n    print(f\"\\n=== Metrics for {name} ===\")\n    for i, col in enumerate(CFG.target_cols):\n        print(f\"{col}: R2 = {r2_score(y_true[:,i], y_pred[:,i]):.4f}\")\n    overall = r2_score(y_true.reshape(-1), y_pred.reshape(-1))\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    wr2 = competition_weighted_r2(y_true, y_pred)\n    print(f\"Overall R2 (flattened): {overall:.4f}\")\n    print(f\"RMSE (all targets): {rmse:.4f}\")\n    print(f\"Competition Weighted R2: {wr2:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint_metrics(\"Swin-Tiny\", targets_swin, preds_swin)\nprint_metrics(\"EffNet-B1\", targets_b1, preds_b1)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.scatter(targets_swin[:,4], preds_swin[:,4], s=6, alpha=0.6)\nplt.xlabel(\"True Dry_Total_g\"); plt.ylabel(\"Pred Dry_Total_g\"); plt.title(\"Swin-Tiny\")\n\nplt.subplot(1,2,2)\nplt.scatter(targets_b1[:,4], preds_b1[:,4], s=6, alpha=0.6)\nplt.xlabel(\"True Dry_Total_g\"); plt.ylabel(\"Pred Dry_Total_g\"); plt.title(\"EffNet-B1\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}